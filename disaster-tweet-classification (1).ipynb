{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**** 1. DATA DESCRIPTION****\n\nGoal - To build a ML model which will classify if a tweet is really about any disaster or not.\n\nData - 10,000 tweets from twitter that are already pre-tagged.\n","metadata":{}},{"cell_type":"markdown","source":"**2. EDA**","metadata":{}},{"cell_type":"markdown","source":"2.1 Let us first load the given data.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nfrom sklearn import model_selection\nfrom IPython.display import display\nfrom collections import defaultdict\nfrom collections import  Counter\n\n# plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tqdm.notebook import tqdm_notebook\n\nfrom wordcloud import STOPWORDS, WordCloud\nfrom termcolor import colored\n\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\ntf.__version__","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:03.807847Z","iopub.execute_input":"2022-07-25T10:47:03.808556Z","iopub.status.idle":"2022-07-25T10:47:06.953460Z","shell.execute_reply.started":"2022-07-25T10:47:03.808416Z","shell.execute_reply":"2022-07-25T10:47:06.951926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:10.882725Z","iopub.execute_input":"2022-07-25T10:47:10.883487Z","iopub.status.idle":"2022-07-25T10:47:10.941037Z","shell.execute_reply.started":"2022-07-25T10:47:10.883453Z","shell.execute_reply":"2022-07-25T10:47:10.939397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Length of train data\", df_train.shape[0])\nprint (\"Length of test data\", df_test.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:13.696212Z","iopub.execute_input":"2022-07-25T10:47:13.696666Z","iopub.status.idle":"2022-07-25T10:47:13.705707Z","shell.execute_reply.started":"2022-07-25T10:47:13.696634Z","shell.execute_reply":"2022-07-25T10:47:13.704448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.2 Target column distribution**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\ndf_train.target.value_counts().plot(kind=\"pie\",\n                                           labels=[\"Disaster(43%)\", \"Not a Disaster(57%)\"],\n                                           colors=['lightcoral','lightskyblue'],\n                                           fontsize=14,\n                                           ylabel=\"\");\n\nplt.subplot(1, 2, 2)\nsns.countplot(x=\"target\",data=df_train, palette=\"RdBu\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:15.688179Z","iopub.execute_input":"2022-07-25T10:47:15.688665Z","iopub.status.idle":"2022-07-25T10:47:15.965622Z","shell.execute_reply.started":"2022-07-25T10:47:15.688617Z","shell.execute_reply":"2022-07-25T10:47:15.964221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.3 Word cloud for each of target value**","metadata":{}},{"cell_type":"code","source":"# Create a list of words for each of the target value by converting their corresponding \n# texts to a list and then removing the stopwords\n\ndisaster = df_train[df_train[\"target\"]==1][\"text\"].tolist()\nnon_disaster = df_train[df_train[\"target\"]==0][\"text\"].to_list()\nprint(\"disaster\",disaster)\nprint(\"Non disaster\",non_disaster)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:18.726476Z","iopub.execute_input":"2022-07-25T10:47:18.726971Z","iopub.status.idle":"2022-07-25T10:47:18.766469Z","shell.execute_reply.started":"2022-07-25T10:47:18.726933Z","shell.execute_reply":"2022-07-25T10:47:18.765234Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Convert(string):\n    words = list(string.lower().split(\" \"))\n    return words","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:24.646420Z","iopub.execute_input":"2022-07-25T10:47:24.646877Z","iopub.status.idle":"2022-07-25T10:47:24.655469Z","shell.execute_reply.started":"2022-07-25T10:47:24.646845Z","shell.execute_reply":"2022-07-25T10:47:24.652544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_df = pd.DataFrame(disaster , columns = [\"text\"])\ndisaster_df[\"List of Words\"] = disaster_df[\"text\"].map(Convert)\n\nnon_disaster_df = pd.DataFrame(non_disaster, columns = [\"text\"])\nnon_disaster_df[\"List of Words\"] = non_disaster_df[\"text\"].map(Convert)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:26.905907Z","iopub.execute_input":"2022-07-25T10:47:26.906355Z","iopub.status.idle":"2022-07-25T10:47:26.940383Z","shell.execute_reply.started":"2022-07-25T10:47:26.906321Z","shell.execute_reply":"2022-07-25T10:47:26.939201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Find the words for disaster and non-disaster after removing the stop words#####\nimport nltk\nstop_words = nltk.corpus.stopwords.words(\"english\")\ndisaster_words = disaster_df[\"List of Words\"]\ndisaster_allwords = []\nfor wordlist in disaster_words:\n    for disaster_word in wordlist:\n        if disaster_word not in stop_words:\n            disaster_allwords.append(disaster_word)\n\n\nnon_disaster_words = non_disaster_df[\"List of Words\"]\nnon_disaster_allwords = []\nfor wordlist in non_disaster_words:\n    for non_disaster_word in wordlist:\n        if non_disaster_word not in stop_words:\n            non_disaster_allwords.append(non_disaster_word)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:28.696189Z","iopub.execute_input":"2022-07-25T10:47:28.696640Z","iopub.status.idle":"2022-07-25T10:47:28.919138Z","shell.execute_reply.started":"2022-07-25T10:47:28.696607Z","shell.execute_reply":"2022-07-25T10:47:28.917809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Disaster Words\",disaster_allwords)\nprint(\"Non-Disaster Words\",non_disaster_allwords)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:31.796460Z","iopub.execute_input":"2022-07-25T10:47:31.797045Z","iopub.status.idle":"2022-07-25T10:47:31.843417Z","shell.execute_reply.started":"2022-07-25T10:47:31.796998Z","shell.execute_reply":"2022-07-25T10:47:31.837964Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us build the word cloud for each of the target by taking most commonly used top 2500 words.\nfrom nltk.probability import FreqDist\nmostcommon_disaster = FreqDist(disaster_allwords).most_common(2500)\nwordcloud = WordCloud(width=1800, height=1000, background_color='white').generate(str(mostcommon_disaster))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Most used words for Disaster ', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:41.403291Z","iopub.execute_input":"2022-07-25T10:47:41.404380Z","iopub.status.idle":"2022-07-25T10:47:46.597814Z","shell.execute_reply.started":"2022-07-25T10:47:41.404331Z","shell.execute_reply":"2022-07-25T10:47:46.595979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mostcommon_nondisaster = FreqDist(disaster_allwords).most_common(2500)\nwordcloud = WordCloud(width=1800, height=1000, background_color='white').generate(str(mostcommon_nondisaster))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Most used words for Non - Disaster ', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:47:52.068730Z","iopub.execute_input":"2022-07-25T10:47:52.069239Z","iopub.status.idle":"2022-07-25T10:47:56.244670Z","shell.execute_reply.started":"2022-07-25T10:47:52.069198Z","shell.execute_reply":"2022-07-25T10:47:56.243247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above word clouds , we can see that the data needs to be cleaned like removing punctuations, url, numbers , emotiocons etc as they will not be helpful. ","metadata":{}},{"cell_type":"markdown","source":"**3. DATA CLEANING ******","metadata":{}},{"cell_type":"code","source":"def clean_dataset(text):\n    text = text.lower()\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '',text) #Removes Websites\n    text  = re.sub(r'<.*?>' ,'', text) \n    text = re.sub(r'\\x89\\S+' , ' ', text) #Removes string starting from \\x89\n    text = re.sub('\\w*\\d\\w*', '', text)  # Removes numbers\n    text = re.sub(r'[^\\w\\s]','',text)   # Removes Punctuations\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:48:01.779919Z","iopub.execute_input":"2022-07-25T10:48:01.780354Z","iopub.status.idle":"2022-07-25T10:48:01.798762Z","shell.execute_reply.started":"2022-07-25T10:48:01.780321Z","shell.execute_reply":"2022-07-25T10:48:01.797066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. DATA PREPARATION**\n\nLet us now clean and prepare our training data.","metadata":{}},{"cell_type":"code","source":"df_train['text'] = df_train['text'].apply(lambda x: clean_dataset(x))","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:48:05.092298Z","iopub.execute_input":"2022-07-25T10:48:05.092694Z","iopub.status.idle":"2022-07-25T10:48:05.413403Z","shell.execute_reply.started":"2022-07-25T10:48:05.092663Z","shell.execute_reply":"2022-07-25T10:48:05.412003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.1 EMBEDDING THE DATASET**","metadata":{}},{"cell_type":"markdown","source":"Let us first create a corpus before embedding our text by breaking the tweets and converting them to indivudual words and then removing the stop words from them.","metadata":{}},{"cell_type":"code","source":"#cleaned_df = pd.DataFrame(df_train , columns = [\"text\"])\ncleaned_df = pd.DataFrame(df_train )\ncleaned_df[\"List of Words\"] = cleaned_df[\"text\"].map(Convert)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:48:08.559658Z","iopub.execute_input":"2022-07-25T10:48:08.560563Z","iopub.status.idle":"2022-07-25T10:48:08.589033Z","shell.execute_reply.started":"2022-07-25T10:48:08.560515Z","shell.execute_reply":"2022-07-25T10:48:08.587787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncleaned_words = cleaned_df[\"List of Words\"]\ncorpus = []\nfor wordlist in cleaned_words:\n    for disaster_word in wordlist:\n        if disaster_word not in stop_words:\n            corpus.append(disaster_word)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:48:11.067590Z","iopub.execute_input":"2022-07-25T10:48:11.067995Z","iopub.status.idle":"2022-07-25T10:48:11.286651Z","shell.execute_reply.started":"2022-07-25T10:48:11.067962Z","shell.execute_reply":"2022-07-25T10:48:11.285196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us embed the dataset using Glove vector. To add the glove vector, use the \"Add data\" in the top right and add the glove vector using the URL \" https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation\".","metadata":{}},{"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word]  =vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:48:13.961976Z","iopub.execute_input":"2022-07-25T10:48:13.962878Z","iopub.status.idle":"2022-07-25T10:48:28.026390Z","shell.execute_reply.started":"2022-07-25T10:48:13.962823Z","shell.execute_reply":"2022-07-25T10:48:28.024846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nMAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:48:43.332809Z","iopub.execute_input":"2022-07-25T10:48:43.333331Z","iopub.status.idle":"2022-07-25T10:48:45.149466Z","shell.execute_reply.started":"2022-07-25T10:48:43.333297Z","shell.execute_reply":"2022-07-25T10:48:45.148050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:48:50.133779Z","iopub.execute_input":"2022-07-25T10:48:50.135055Z","iopub.status.idle":"2022-07-25T10:48:50.143933Z","shell.execute_reply.started":"2022-07-25T10:48:50.135018Z","shell.execute_reply":"2022-07-25T10:48:50.141614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nnum_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words, 100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:48:52.944053Z","iopub.execute_input":"2022-07-25T10:48:52.944555Z","iopub.status.idle":"2022-07-25T10:48:53.010773Z","shell.execute_reply.started":"2022-07-25T10:48:52.944522Z","shell.execute_reply":"2022-07-25T10:48:53.009340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. MODEL BUILDING**\n\nLet us build an LSTM model. \n\nMODEL ARCHITECTURE :\n\nOur model consists of three layers :\n* one embedding layer\n* one LSTM layer and \n* one output layer.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Bidirectional,Flatten,Dropout\nfrom keras.initializers import Constant\nfrom tensorflow.keras.optimizers import Adam\nmodel = Sequential()\nmodel.add(Embedding(num_words,100,input_length=MAX_LEN,weights=[embedding_matrix]))\nmodel.add(Bidirectional(LSTM(12,dropout=0.2,return_sequences=False)))\nmodel.add(Flatten())\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2,activation='softmax'))\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:48:56.496440Z","iopub.execute_input":"2022-07-25T10:48:56.497806Z","iopub.status.idle":"2022-07-25T10:48:58.288818Z","shell.execute_reply.started":"2022-07-25T10:48:56.497738Z","shell.execute_reply":"2022-07-25T10:48:58.287552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimzer = Adam(learning_rate=1e-5)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=optimzer,metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:49:01.544430Z","iopub.execute_input":"2022-07-25T10:49:01.544881Z","iopub.status.idle":"2022-07-25T10:49:01.564220Z","shell.execute_reply.started":"2022-07-25T10:49:01.544847Z","shell.execute_reply":"2022-07-25T10:49:01.562795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us split the data set into train and validation sets and use them for model training.","metadata":{}},{"cell_type":"code","source":"#epochs = 50\n#batch_size = 32\n#history = model.fit(x=after_padding_sequence,y=y,validation_split=0.2,epochs=epochs,batch_size=batch_size,verbose=1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(type(df_train['target'].values))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain=tweet_pad[:cleaned_df.shape[0]]\nprint(train.shape)\nprint(cleaned_df['target'].values.shape)\nX_train,X_test,y_train,y_test = train_test_split(train,cleaned_df['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:49:05.272922Z","iopub.execute_input":"2022-07-25T10:49:05.273398Z","iopub.status.idle":"2022-07-25T10:49:05.284908Z","shell.execute_reply.started":"2022-07-25T10:49:05.273365Z","shell.execute_reply":"2022-07-25T10:49:05.283352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,\n                  validation_data=(X_test,y_test),verbose=2)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:49:09.837705Z","iopub.execute_input":"2022-07-25T10:49:09.839609Z","iopub.status.idle":"2022-07-25T10:53:35.266116Z","shell.execute_reply.started":"2022-07-25T10:49:09.839558Z","shell.execute_reply":"2022-07-25T10:53:35.264730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6.**INFERENCE****","metadata":{}},{"cell_type":"code","source":"import gc\ngc.collect()\nplt.figure(figsize=(20,5))\nplt.plot(history.history['loss'],'-o',label=\"train\")\nplt.plot(history.history['val_loss'],'-o',label=\"Validation\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss change over epoch for Model\")\nplt.legend()\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:53:43.878068Z","iopub.execute_input":"2022-07-25T10:53:43.878560Z","iopub.status.idle":"2022-07-25T10:53:44.518490Z","shell.execute_reply.started":"2022-07-25T10:53:43.878527Z","shell.execute_reply":"2022-07-25T10:53:44.516936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\nplt.figure(figsize=(20,5))\nplt.plot(history.history['accuracy'],label=\"train\")\nplt.plot(history.history['val_accuracy'],label=\"Validation\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy change over epoch\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:55:37.059616Z","iopub.execute_input":"2022-07-25T10:55:37.060050Z","iopub.status.idle":"2022-07-25T10:55:37.593533Z","shell.execute_reply.started":"2022-07-25T10:55:37.060003Z","shell.execute_reply":"2022-07-25T10:55:37.592199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_df_test = pd.DataFrame(df_test )\ntest_padded =  tweet_pad[:cleaned_df_test.shape[0]]\npred = model.predict(test_padded)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:55:06.995753Z","iopub.execute_input":"2022-07-25T10:55:06.996487Z","iopub.status.idle":"2022-07-25T10:55:08.117266Z","shell.execute_reply.started":"2022-07-25T10:55:06.996437Z","shell.execute_reply":"2022-07-25T10:55:08.115897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor p in pred:\n  preds.append(np.argmax(p))","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:55:52.419620Z","iopub.execute_input":"2022-07-25T10:55:52.420052Z","iopub.status.idle":"2022-07-25T10:55:52.438710Z","shell.execute_reply.started":"2022-07-25T10:55:52.420022Z","shell.execute_reply":"2022-07-25T10:55:52.437562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_csv = pd.DataFrame()\nsubmission_csv['id'] = cleaned_df_test['id']\nsubmission_csv['target'] = preds","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:57:09.356529Z","iopub.execute_input":"2022-07-25T10:57:09.356956Z","iopub.status.idle":"2022-07-25T10:57:09.369976Z","shell.execute_reply.started":"2022-07-25T10:57:09.356925Z","shell.execute_reply":"2022-07-25T10:57:09.368646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_csv.to_csv(f'/kaggle/working/submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:57:21.684746Z","iopub.execute_input":"2022-07-25T10:57:21.686002Z","iopub.status.idle":"2022-07-25T10:57:21.702087Z","shell.execute_reply.started":"2022-07-25T10:57:21.685953Z","shell.execute_reply":"2022-07-25T10:57:21.700776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_csv.head(25)","metadata":{"execution":{"iopub.status.busy":"2022-07-25T10:57:42.387859Z","iopub.execute_input":"2022-07-25T10:57:42.388459Z","iopub.status.idle":"2022-07-25T10:57:42.407119Z","shell.execute_reply.started":"2022-07-25T10:57:42.388411Z","shell.execute_reply":"2022-07-25T10:57:42.405451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FUTURE DEVLOPMENTS**\n\nThe accuracy of the model can be further increased by :\n\n* Increasing the number of epochs\n* Adding more data cleaning steps like spell check etc\n* Using different word embeddings or advanced models like BERT, Auto-encoders etc.\n* We can also increase the number of memory cells in the LSTM network or try different optmisers like PRELU or changing the drop out etc.","metadata":{}}]}